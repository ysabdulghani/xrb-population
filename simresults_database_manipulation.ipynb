{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FULL CSV Files:  58%|█████▊    | 839/1458 [01:32<01:31,  6.73it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 5589   -179.019997\n",
      "dtype: float64 ... 5589    18.009702\n",
      "Name: error_d_up, dtype: float64 ... 5589    197.029699\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files:  82%|████████▏ | 1189/1458 [02:13<00:32,  8.20it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 1914   -3.31801\n",
      "dtype: float64 ... 1914    6.210822\n",
      "Name: error_d_up, dtype: float64 ... 1914    9.528832\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files:  88%|████████▊ | 1277/1458 [02:22<00:17, 10.26it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 567   -0.912259\n",
      "dtype: float64 ... 567    1.542494\n",
      "Name: error_d_up, dtype: float64 ... 567    2.454753\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files:  90%|████████▉ | 1307/1458 [02:26<00:22,  6.78it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 6679   -0.26781\n",
      "dtype: float64 ... 6679    2.570955\n",
      "Name: error_d_up, dtype: float64 ... 6679    2.838765\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files:  91%|█████████ | 1324/1458 [02:28<00:19,  6.92it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 7431   -0.729544\n",
      "dtype: float64 ... 7431    4.446376\n",
      "Name: error_d_up, dtype: float64 ... 7431    5.17592\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files:  94%|█████████▍| 1370/1458 [02:35<00:12,  7.01it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 6350   -0.586249\n",
      "dtype: float64 ... 6350    1.573424\n",
      "Name: error_d_up, dtype: float64 ... 6350    2.159673\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files:  99%|█████████▉| 1449/1458 [02:44<00:01,  8.66it/s]/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 787   -0.46607\n",
      "dtype: float64 ... 787    2.47468\n",
      "Name: error_d_up, dtype: float64 ... 787    2.94075\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_755700/3024815655.py:109: UserWarning: Non-positive error intervals detected: 4165   -1.337518\n",
      "dtype: float64 ... 4165    2.763465\n",
      "Name: error_d_up, dtype: float64 ... 4165    4.100983\n",
      "Name: error_d_low, dtype: float64 will take absolute of the difference\n",
      "  warnings.warn(\n",
      "Processing FULL CSV Files: 100%|██████████| 1458/1458 [02:46<00:00,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Only missing columns were computed for existing aggregated files.\n",
      "Collected 36221 valid distances and 22099 missing distances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# For progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable, desc=\"\"):\n",
    "        return iterable\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Configuration / Setup\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 1a) Suppress warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The number of bins estimated may be suboptimal.\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=DeprecationWarning,\n",
    "    message=\"DataFrameGroupBy.apply operated on the grouping columns\"\n",
    ")\n",
    "\n",
    "# 1b) Instrument, file patterns, output directory\n",
    "instr = 'maxi'\n",
    "os.makedirs(f\"plots/{instr}\", exist_ok=True)\n",
    "\n",
    "file_pattern_full = (\n",
    "    f'/disk/data/youssef/scripts/xrb-population/results_latest/{instr}_results/*full*.csv'\n",
    ")\n",
    "file_pattern_agg  = (\n",
    "    f'/disk/data/youssef/scripts/xrb-population/results_latest/{instr}_results/*.csv'\n",
    ")\n",
    "\n",
    "# Identify the files\n",
    "csv_files_full = [f for f in glob(file_pattern_full)]\n",
    "csv_files_agg  = [f for f in glob(file_pattern_agg) if \"full\" not in f]\n",
    "\n",
    "# The columns we ultimately want in the aggregated file\n",
    "all_needed_cols = [\n",
    "    \"red_chi_squared\", \"gamma\", \"temp\", \"power_norm_fit\",\n",
    "    \"disk_norm_fit\", \"total_flux_median\", \"d_fit_median\",\n",
    "    \"d_fit_peak\", \"peak_flux\", \"d_fit_weighted_median\",\n",
    "    \"error_d_median\", \"frac_uncert_median\", \"error_d_peak\",\n",
    "    \"frac_uncert_peak\"\n",
    "]\n",
    "\n",
    "# This is the central column (distance statistic) used for filtering\n",
    "central_value = \"d_fit_weighted_median\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Global Data Structures\n",
    "# ---------------------------------------------------------------------\n",
    "# Same structures from your original script\n",
    "distances       = []\n",
    "na_distances    = []\n",
    "tight_distances = []\n",
    "na_nH           = []\n",
    "all_data        = []\n",
    "na_d_fit_files  = []\n",
    "na_d_fit_params = []\n",
    "all_data_params = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Helper Functions\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_parameters(filename):\n",
    "    \"\"\"\n",
    "    Extract parameter values (g, T, a, m, i, r, e) from the filename \n",
    "    using regular expressions. Adjust the pattern to match your filenames.\n",
    "    \"\"\"\n",
    "    pattern = (\n",
    "        r'table_g(?P<g>\\d+\\.\\d+)_T(?P<T>\\d+\\.\\d+)_a(?P<a>\\d+\\.\\d+)_'\n",
    "        r'm(?P<m>\\d+\\.\\d+)_i(?P<i>\\d+\\.\\d+)_r(?P<r>\\d+\\.\\d+)_e(?P<e>\\d+\\.\\d+)'\n",
    "    )\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return {key: float(value) for key, value in match.groupdict().items()}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calc_MAE(scale, l_err, u_err):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error (MAE) and safeguard reciprocal weight calculation.\n",
    "\n",
    "    Parameters:\n",
    "        scale (float): The scaling factor for error normalization.\n",
    "        l_err (numpy.ndarray): Lower error values.\n",
    "        u_err (numpy.ndarray): Upper error values.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The computed MAE array with safeguards.\n",
    "    \"\"\"\n",
    "    # Check that l_err and u_err are of the same length\n",
    "    if len(l_err) != len(u_err):\n",
    "        raise ValueError(\"`l_err` and `u_err` must have the same length.\")\n",
    "    \n",
    "    # Ensure no negative or zero error intervals\n",
    "    error_diff = u_err - l_err\n",
    "    if np.any(error_diff <= 0):\n",
    "        warnings.warn(\n",
    "            f\"Non-positive error intervals detected: {error_diff[error_diff <= 0]} ... {u_err[error_diff <= 0]} ... {l_err[error_diff <= 0]} will take absolute of the difference\")\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = (error_diff / 2) / scale\n",
    "\n",
    "    # Check for invalid or NaN results\n",
    "    if np.any(np.isnan(mae)):\n",
    "        raise ValueError(\"MAE calculation resulted in NaN values.\")\n",
    "    \n",
    "    return np.abs(mae)\n",
    "\n",
    "\n",
    "def weighted_median(values, weights):\n",
    "    \"\"\"\n",
    "    Compute the weighted median of a 1D NumPy array `values` \n",
    "    with positive `weights`.\n",
    "\n",
    "    Parameters:\n",
    "        values (numpy.ndarray): Array of data values.\n",
    "        weights (numpy.ndarray): Array of weights corresponding to `values`.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The weighted median, or None if inputs are invalid.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(values, np.ndarray) or not isinstance(weights, np.ndarray):\n",
    "        raise TypeError(\"Both `values` and `weights` must be NumPy arrays.\")\n",
    "    if values.ndim != 1 or weights.ndim != 1:\n",
    "        raise ValueError(\"Both `values` and `weights` must be 1D arrays.\")\n",
    "    if len(values) == 0:\n",
    "        raise ValueError(\"`values` array cannot be empty.\")\n",
    "    if len(values) != len(weights):\n",
    "        raise ValueError(\"`values` and `weights` must have the same length.\")\n",
    "    if np.any(weights < 0):\n",
    "        raise ValueError(f\"`weights` must contain only non-negative values. {weights[weights < 0]}\")\n",
    "    if np.any(np.isnan(weights)):\n",
    "        raise ValueError(\"`weights` cannot contain NaN values.\")\n",
    "    if np.sum(weights) == 0:\n",
    "        raise ValueError(\"Sum of `weights` must be greater than zero.\")\n",
    "\n",
    "    # Sort values and weights by `values`\n",
    "    sort_idx = np.argsort(values)\n",
    "    sorted_values = values[sort_idx]\n",
    "    sorted_weights = weights[sort_idx]\n",
    "\n",
    "    # Compute cumulative weights\n",
    "    cumulative_weights = np.cumsum(sorted_weights)\n",
    "    total_weight = cumulative_weights[-1]\n",
    "    midpoint = 0.5 * total_weight\n",
    "\n",
    "    # Find the index where cumulative weight crosses the midpoint\n",
    "    median_idx = np.searchsorted(cumulative_weights, midpoint, side='right')\n",
    "\n",
    "    return sorted_values[median_idx]\n",
    "\n",
    "def find_peak(array):\n",
    "    \"\"\"\n",
    "    Find the peak (mode) of the distribution by computing a histogram.\n",
    "    The bin size uses 'stone' by default. Adjust if needed.\n",
    "    \"\"\"\n",
    "    if len(array) == 0:\n",
    "        return None\n",
    "    counts, bins = np.histogram(array, bins='stone')\n",
    "    peak_idx = np.argmax(counts)\n",
    "    return 0.5 * (bins[peak_idx] + bins[peak_idx + 1])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Function to compute ALL columns\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_all_stats(df):\n",
    "    \"\"\"\n",
    "    Compute all columns for each group (nH, d).\n",
    "    \"\"\"\n",
    "    nH_val = df['nH'].iloc[0] if 'nH' in df.columns else None\n",
    "    d_val  = df['d'].iloc[0]  if 'd'  in df.columns else None\n",
    "\n",
    "    d_fit_peak = find_peak(df['d_fit'].dropna()) if 'd_fit' in df.columns else None\n",
    "    total_flux_peak = find_peak(df['total_flux'].dropna()) if 'total_flux' in df.columns else None\n",
    "\n",
    "    if {'d_fit', 'error_d_low', 'error_d_up'}.issubset(df.columns):\n",
    "        valid_mask = (\n",
    "            df['d_fit'].notna() &\n",
    "            df['error_d_low'].notna() &\n",
    "            df['error_d_up'].notna()\n",
    "        )\n",
    "        if valid_mask.any():\n",
    "            w = 1 / calc_MAE(\n",
    "                df.loc[valid_mask, 'd_fit'],\n",
    "                df.loc[valid_mask, 'error_d_low'],\n",
    "                df.loc[valid_mask, 'error_d_up']\n",
    "            )\n",
    "            d_fit_weighted_med = weighted_median(\n",
    "                df.loc[valid_mask, 'd_fit'].values,\n",
    "                w.values\n",
    "            )\n",
    "        else:\n",
    "            d_fit_weighted_med = None\n",
    "    else:\n",
    "        d_fit_weighted_med = None\n",
    "\n",
    "    if d_fit_peak is not None and d_val is not None:\n",
    "        peak_dist_err = d_fit_peak - d_val\n",
    "        frac_peak_err = peak_dist_err / d_val if d_val != 0 else None\n",
    "    else:\n",
    "        peak_dist_err = None\n",
    "        frac_peak_err = None\n",
    "\n",
    "    if 'd_fit' in df.columns and df['d_fit'].notna().any() and d_val is not None:\n",
    "        d_fit_median = df['d_fit'].median()\n",
    "        error_d_median = d_fit_median - d_val\n",
    "        frac_uncert_median = (\n",
    "            error_d_median / d_val \n",
    "            if d_val != 0 else None\n",
    "        )\n",
    "    else:\n",
    "        d_fit_median = None\n",
    "        error_d_median = None\n",
    "        frac_uncert_median = None\n",
    "\n",
    "    return pd.Series({\n",
    "        \"nH\": nH_val,\n",
    "        \"d\": d_val,\n",
    "\n",
    "        \"red_chi_squared\": df['red_chi_squared'].median() if 'red_chi_squared' in df.columns else None,\n",
    "        \"gamma\": df['gamma'].median() if 'gamma' in df.columns else None,\n",
    "        \"temp\": df['temp'].median() if 'temp' in df.columns else None,\n",
    "\n",
    "        \"power_norm_fit\": df['power_norm_fit'].median() if 'power_norm_fit' in df.columns else None,\n",
    "        \"disk_norm_fit\": df['disk_norm_fit'].median() if 'disk_norm_fit' in df.columns else None,\n",
    "        \"total_flux_median\": df['total_flux'].median() if 'total_flux' in df.columns else None,\n",
    "\n",
    "        \"d_fit_median\": d_fit_median,\n",
    "        \"d_fit_peak\": d_fit_peak,\n",
    "        \"peak_flux\": total_flux_peak,\n",
    "        \"d_fit_weighted_median\": d_fit_weighted_med,\n",
    "\n",
    "        \"error_d_median\": error_d_median,\n",
    "        \"frac_uncert_median\": frac_uncert_median,\n",
    "        \"error_d_peak\": peak_dist_err,\n",
    "        \"frac_uncert_peak\": frac_peak_err\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Function to compute ONLY a subset of columns\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_subset_stats(df, needed_cols):\n",
    "    \"\"\"\n",
    "    Compute only the columns in `needed_cols` for each group (nH, d).\n",
    "    Return them plus (nH, d).\n",
    "    \"\"\"\n",
    "    nH_val = df['nH'].iloc[0] if 'nH' in df.columns else None\n",
    "    d_val  = df['d'].iloc[0]  if 'd'  in df.columns else None\n",
    "\n",
    "    result = {\n",
    "        \"nH\": nH_val,\n",
    "        \"d\": d_val,\n",
    "    }\n",
    "\n",
    "    # Example logic for 'd_fit_peak', 'error_d_peak', 'frac_uncert_peak'\n",
    "    if \"d_fit_peak\" in needed_cols or \"error_d_peak\" in needed_cols or \"frac_uncert_peak\" in needed_cols:\n",
    "        d_fit_peak = find_peak(df['d_fit'].dropna()) if 'd_fit' in df.columns else None\n",
    "        if \"d_fit_peak\" in needed_cols:\n",
    "            result[\"d_fit_peak\"] = d_fit_peak\n",
    "        if d_fit_peak is not None and d_val is not None:\n",
    "            peak_dist_err = d_fit_peak - d_val\n",
    "            frac_peak_err = peak_dist_err / d_val if d_val != 0 else None\n",
    "        else:\n",
    "            peak_dist_err = None\n",
    "            frac_peak_err = None\n",
    "        if \"error_d_peak\" in needed_cols:\n",
    "            result[\"error_d_peak\"] = peak_dist_err\n",
    "        if \"frac_uncert_peak\" in needed_cols:\n",
    "            result[\"frac_uncert_peak\"] = frac_peak_err\n",
    "\n",
    "    if \"peak_flux\" in needed_cols:\n",
    "        result[\"peak_flux\"] = find_peak(df['total_flux'].dropna()) if 'total_flux' in df.columns else None\n",
    "\n",
    "    if \"d_fit_weighted_median\" in needed_cols:\n",
    "        if {'d_fit', 'error_d_low', 'error_d_up'}.issubset(df.columns):\n",
    "            valid_mask = (\n",
    "                df['d_fit'].notna() &\n",
    "                df['error_d_low'].notna() &\n",
    "                df['error_d_up'].notna()\n",
    "            )\n",
    "            if valid_mask.any():\n",
    "                w = 1 / calc_MAE(\n",
    "                    df.loc[valid_mask, 'd_fit'],\n",
    "                    df.loc[valid_mask, 'error_d_low'],\n",
    "                    df.loc[valid_mask, 'error_d_up']\n",
    "                )\n",
    "                result[\"d_fit_weighted_median\"] = weighted_median(\n",
    "                    df.loc[valid_mask, 'd_fit'].values,\n",
    "                    w.values\n",
    "                )\n",
    "            else:\n",
    "                result[\"d_fit_weighted_median\"] = None\n",
    "        else:\n",
    "            result[\"d_fit_weighted_median\"] = None\n",
    "\n",
    "    if (\"d_fit_median\" in needed_cols or \n",
    "        \"error_d_median\" in needed_cols or \n",
    "        \"frac_uncert_median\" in needed_cols):\n",
    "        \n",
    "        if 'd_fit' in df.columns and df['d_fit'].notna().any() and d_val is not None:\n",
    "            d_fit_median = df['d_fit'].median()\n",
    "            error_d_median = d_fit_median - d_val\n",
    "            frac_uncert_median = (error_d_median / d_val) if d_val != 0 else None\n",
    "        else:\n",
    "            d_fit_median = None\n",
    "            error_d_median = None\n",
    "            frac_uncert_median = None\n",
    "\n",
    "        if \"d_fit_median\" in needed_cols:\n",
    "            result[\"d_fit_median\"] = d_fit_median\n",
    "        if \"error_d_median\" in needed_cols:\n",
    "            result[\"error_d_median\"] = error_d_median\n",
    "        if \"frac_uncert_median\" in needed_cols:\n",
    "            result[\"frac_uncert_median\"] = frac_uncert_median\n",
    "\n",
    "    if \"red_chi_squared\" in needed_cols:\n",
    "        result[\"red_chi_squared\"] = df['red_chi_squared'].median() if 'red_chi_squared' in df.columns else None\n",
    "\n",
    "    if \"gamma\" in needed_cols:\n",
    "        result[\"gamma\"] = df['gamma'].median() if 'gamma' in df.columns else None\n",
    "\n",
    "    if \"temp\" in needed_cols:\n",
    "        result[\"temp\"] = df['temp'].median() if 'temp' in df.columns else None\n",
    "\n",
    "    if \"power_norm_fit\" in needed_cols:\n",
    "        result[\"power_norm_fit\"] = df['power_norm_fit'].median() if 'power_norm_fit' in df.columns else None\n",
    "\n",
    "    if \"disk_norm_fit\" in needed_cols:\n",
    "        result[\"disk_norm_fit\"] = df['disk_norm_fit'].median() if 'disk_norm_fit' in df.columns else None\n",
    "\n",
    "    if \"total_flux_median\" in needed_cols:\n",
    "        result[\"total_flux_median\"] = df['total_flux'].median() if 'total_flux' in df.columns else None\n",
    "\n",
    "    return pd.Series(result)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Main Loop: Only Compute Missing Columns, Collect Data Structures\n",
    "# ---------------------------------------------------------------------\n",
    "merged_results = []\n",
    "\n",
    "for file in tqdm(csv_files_full, desc=\"Processing FULL CSV Files\"):\n",
    "    # 6a) Extract parameters\n",
    "    params = extract_parameters(file)\n",
    "    if params is None:\n",
    "        continue\n",
    "\n",
    "    base_name_full = os.path.basename(file)\n",
    "    base_name_no_ext = os.path.splitext(base_name_full)[0].replace(\"_full\", \"\")\n",
    "\n",
    "    # 6b) Attempt to find existing aggregated file\n",
    "    existing_agg = None\n",
    "    for f_agg in csv_files_agg:\n",
    "        if base_name_no_ext in f_agg:\n",
    "            try:\n",
    "                existing_agg = pd.read_csv(f_agg)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {f_agg}: {e}\")\n",
    "                existing_agg = None\n",
    "\n",
    "    # 6c) Load the FULL CSV data\n",
    "    try:\n",
    "        full_data = pd.read_csv(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Must have 'nH' and 'd' for grouping\n",
    "    if 'nH' not in full_data.columns or 'd' not in full_data.columns:\n",
    "        continue\n",
    "\n",
    "    # 6d) If no existing aggregated file, compute ALL columns\n",
    "    if existing_agg is None:\n",
    "        aggregated_df = (\n",
    "            full_data\n",
    "            .groupby(['nH', 'd'], as_index=False)\n",
    "            .apply(compute_all_stats)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        # Identify which columns are missing\n",
    "        missing_cols = [c for c in all_needed_cols if c not in existing_agg.columns]\n",
    "        if not missing_cols:\n",
    "            # Everything is already present\n",
    "            aggregated_df = existing_agg.copy()\n",
    "        else:\n",
    "            # Compute only the missing columns\n",
    "            partial_agg = (\n",
    "                full_data\n",
    "                .groupby(['nH', 'd'], as_index=False)\n",
    "                .apply(lambda grp: compute_subset_stats(grp, missing_cols))\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            # Merge partial_agg with existing_agg on (nH, d)\n",
    "            aggregated_df = pd.merge(\n",
    "                existing_agg,\n",
    "                partial_agg,\n",
    "                on=[\"nH\", \"d\"],\n",
    "                how=\"outer\",\n",
    "                suffixes=(\"_old\", \"\")\n",
    "            )\n",
    "            # Fill missing columns from partial_agg\n",
    "            for col in missing_cols:\n",
    "                old_col_name = col + \"_old\"\n",
    "                new_col_name = col\n",
    "                if old_col_name in aggregated_df.columns and new_col_name in aggregated_df.columns:\n",
    "                    # Fill the old column's NaNs with new column's values\n",
    "                    mask_na = aggregated_df[old_col_name].isna()\n",
    "                    aggregated_df.loc[mask_na, old_col_name] = aggregated_df.loc[mask_na, new_col_name]\n",
    "                    # Drop new col, rename old -> col\n",
    "                    aggregated_df.drop(columns=[new_col_name], inplace=True)\n",
    "                    aggregated_df.rename(columns={old_col_name: col}, inplace=True)\n",
    "                elif old_col_name in aggregated_df.columns and new_col_name not in aggregated_df.columns:\n",
    "                    # The new aggregator didn't produce that col, rename old -> new\n",
    "                    aggregated_df.rename(columns={old_col_name: col}, inplace=True)\n",
    "\n",
    "            # Remove leftover columns with _old suffix\n",
    "            leftover_old = [c for c in aggregated_df.columns if c.endswith(\"_old\")]\n",
    "            aggregated_df.drop(columns=leftover_old, inplace=True)\n",
    "\n",
    "    # 6e) Collect data for distances, na_distances, etc.\n",
    "    # Filter rows where `central_value` is present\n",
    "    valid_data = aggregated_df[aggregated_df[central_value].notna()]\n",
    "    distances.extend(valid_data['d'].values)\n",
    "\n",
    "    filtered_data = valid_data[(np.abs(valid_data['d']-valid_data[central_value])/valid_data['d']) <= 0.5]\n",
    "    tight_distances.extend(filtered_data['d'].values)\n",
    "\n",
    "    # Rows where `central_value` is NaN\n",
    "    na_data = aggregated_df[aggregated_df[central_value].isna()]\n",
    "    na_distances.extend(na_data['d'].values)\n",
    "    na_nH.extend(na_data['nH'].values)\n",
    "\n",
    "    # Keep the entire aggregated data in `all_data`\n",
    "    all_data.append(aggregated_df)\n",
    "    all_data_params.append(params)\n",
    "\n",
    "    # Check if 'd' == 1 for missing `central_value`\n",
    "    selected_data = aggregated_df[aggregated_df['d'] == 1]\n",
    "    if aggregated_df[central_value].isna().any():\n",
    "        na_d_fit_files.append(os.path.basename(file))\n",
    "\n",
    "    na_d_fit_params.extend([params]*len(na_data['d'].values))\n",
    "\n",
    "    merged_results.append(aggregated_df)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) Combine everything (optional) and finalize\n",
    "# ---------------------------------------------------------------------\n",
    "final_merged_df = pd.concat(merged_results, ignore_index=True)\n",
    "\n",
    "# If you want to save the final result:\n",
    "# final_merged_df.to_csv(\"final_merged_aggregated_stats.csv\", index=False)\n",
    "\n",
    "print(\"Done. Only missing columns were computed for existing aggregated files.\")\n",
    "print(f\"Collected {len(distances)} valid distances and {len(na_distances)} missing distances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_with_params = []\n",
    "for data_df, params in zip(all_data, all_data_params):  # Match each dataset with its parameters\n",
    "    # Repeat the parameter dictionary for each row in the corresponding data_df\n",
    "    params_df = pd.DataFrame([params] * len(data_df))\n",
    "    # Concatenate data and parameters\n",
    "    data_with_params = pd.concat([data_df.reset_index(drop=True), params_df.reset_index(drop=True)], axis=1)\n",
    "    all_data_with_params.append(data_with_params)\n",
    "\n",
    "# Step 2: Concatenate all entries into a single DataFrame\n",
    "all_data_flat = pd.concat(all_data_with_params, ignore_index=True)\n",
    "all_data_flat['diff_d'] = all_data_flat['d_fit_weighted_median'] - all_data_flat['d']\n",
    "\n",
    "all_data_flat['instr']= instr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened SQLite database with sqlalchemy version 2.0.30 successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "db = sqlalchemy.create_engine('sqlite:///results.db')\n",
    "\n",
    "try:\n",
    "    with db.connect() as conn:\n",
    "        print(f\"Opened SQLite database with sqlalchemy version {sqlalchemy.__version__} successfully.\")\n",
    "        xrt_results = pd.read_sql('SELECT * FROM maxi',conn)\n",
    "except Exception as e:\n",
    "    print(\"Failed to open database:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
