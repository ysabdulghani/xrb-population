{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# For progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable, desc=\"\"):\n",
    "        return iterable\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Configuration / Setup\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 1a) Suppress warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The number of bins estimated may be suboptimal.\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=DeprecationWarning,\n",
    "    message=\"DataFrameGroupBy.apply operated on the grouping columns\"\n",
    ")\n",
    "\n",
    "# 1b) Instrument, file patterns, output directory\n",
    "instr = 'maxi'\n",
    "os.makedirs(f\"plots/{instr}\", exist_ok=True)\n",
    "\n",
    "file_pattern_full = (\n",
    "    f'/disk/data/youssef/scripts/xrb-population/results_latest/{instr}_results/*full*.csv'\n",
    ")\n",
    "file_pattern_agg  = (\n",
    "    f'/disk/data/youssef/scripts/xrb-population/results_latest/{instr}_results/*.csv'\n",
    ")\n",
    "\n",
    "# Identify the files\n",
    "csv_files_full = [f for f in glob(file_pattern_full)]\n",
    "csv_files_agg  = [f for f in glob(file_pattern_agg) if \"full\" not in f]\n",
    "\n",
    "# The columns we ultimately want in the aggregated file\n",
    "all_needed_cols = [\n",
    "    \"red_chi_squared\", \"gamma\", \"temp\", \"power_norm_fit\",\n",
    "    \"disk_norm_fit\", \"total_flux_median\", \"d_fit_median\",\n",
    "    \"d_fit_peak\", \"peak_flux\", \"d_fit_weighted_median\",\n",
    "    \"error_d_median\", \"frac_uncert_median\", \"error_d_peak\",\n",
    "    \"frac_uncert_peak\"\n",
    "]\n",
    "\n",
    "# This is the central column (distance statistic) used for filtering\n",
    "central_value = \"d_fit_weighted_median\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Global Data Structures\n",
    "# ---------------------------------------------------------------------\n",
    "# Same structures from your original script\n",
    "distances       = []\n",
    "na_distances    = []\n",
    "na_nH           = []\n",
    "all_data        = []\n",
    "na_d_fit_files  = []\n",
    "na_d_fit_params = []\n",
    "all_data_params = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Helper Functions\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_parameters(filename):\n",
    "    \"\"\"\n",
    "    Extract parameter values (g, T, a, m, i, r, e) from the filename \n",
    "    using regular expressions. Adjust the pattern to match your filenames.\n",
    "    \"\"\"\n",
    "    pattern = (\n",
    "        r'table_g(?P<g>\\d+\\.\\d+)_T(?P<T>\\d+\\.\\d+)_a(?P<a>\\d+\\.\\d+)_'\n",
    "        r'm(?P<m>\\d+\\.\\d+)_i(?P<i>\\d+\\.\\d+)_r(?P<r>\\d+\\.\\d+)_e(?P<e>\\d+\\.\\d+)'\n",
    "    )\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return {key: float(value) for key, value in match.groupdict().items()}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calc_MAE(scale, l_err, u_err):\n",
    "    \"\"\"Calculate Mean Absolute Error (MAE).\"\"\"\n",
    "    return ((u_err - l_err) / 2)/scale\n",
    "\n",
    "def weighted_median(values, weights):\n",
    "    \"\"\"\n",
    "    Compute the weighted median of a 1D NumPy array `values` \n",
    "    with positive `weights`.\n",
    "    \"\"\"\n",
    "    if len(values) == 0:\n",
    "        return None\n",
    "\n",
    "    sort_idx = np.argsort(values)\n",
    "    sorted_values = values[sort_idx]\n",
    "    sorted_weights = weights[sort_idx]\n",
    "\n",
    "    cumulative_weights = np.cumsum(sorted_weights)\n",
    "    total_weight = cumulative_weights[-1]\n",
    "    midpoint = 0.5 * total_weight\n",
    "    median_idx = np.searchsorted(cumulative_weights, midpoint, side='right')\n",
    "    return sorted_values[median_idx]\n",
    "\n",
    "def find_peak(array):\n",
    "    \"\"\"\n",
    "    Find the peak (mode) of the distribution by computing a histogram.\n",
    "    The bin size uses 'stone' by default. Adjust if needed.\n",
    "    \"\"\"\n",
    "    if len(array) == 0:\n",
    "        return None\n",
    "    counts, bins = np.histogram(array, bins='stone')\n",
    "    peak_idx = np.argmax(counts)\n",
    "    return 0.5 * (bins[peak_idx] + bins[peak_idx + 1])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Function to compute ALL columns\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_all_stats(df):\n",
    "    \"\"\"\n",
    "    Compute all columns for each group (nH, d).\n",
    "    \"\"\"\n",
    "    nH_val = df['nH'].iloc[0] if 'nH' in df.columns else None\n",
    "    d_val  = df['d'].iloc[0]  if 'd'  in df.columns else None\n",
    "\n",
    "    d_fit_peak = find_peak(df['d_fit'].dropna()) if 'd_fit' in df.columns else None\n",
    "    total_flux_peak = find_peak(df['total_flux'].dropna()) if 'total_flux' in df.columns else None\n",
    "\n",
    "    if {'d_fit', 'error_d_low', 'error_d_up'}.issubset(df.columns):\n",
    "        valid_mask = (\n",
    "            df['d_fit'].notna() &\n",
    "            df['error_d_low'].notna() &\n",
    "            df['error_d_up'].notna()\n",
    "        )\n",
    "        if valid_mask.any():\n",
    "            w = 1 / calc_MAE(\n",
    "                df.loc[valid_mask, 'd_fit'],\n",
    "                df.loc[valid_mask, 'error_d_low'],\n",
    "                df.loc[valid_mask, 'error_d_up']\n",
    "            )\n",
    "            d_fit_weighted_med = weighted_median(\n",
    "                df.loc[valid_mask, 'd_fit'].values,\n",
    "                w.values\n",
    "            )\n",
    "        else:\n",
    "            d_fit_weighted_med = None\n",
    "    else:\n",
    "        d_fit_weighted_med = None\n",
    "\n",
    "    if d_fit_peak is not None and d_val is not None:\n",
    "        peak_dist_err = d_fit_peak - d_val\n",
    "        frac_peak_err = peak_dist_err / d_val if d_val != 0 else None\n",
    "    else:\n",
    "        peak_dist_err = None\n",
    "        frac_peak_err = None\n",
    "\n",
    "    if 'd_fit' in df.columns and df['d_fit'].notna().any() and d_val is not None:\n",
    "        d_fit_median = df['d_fit'].median()\n",
    "        error_d_median = d_fit_median - d_val\n",
    "        frac_uncert_median = (\n",
    "            error_d_median / d_val \n",
    "            if d_val != 0 else None\n",
    "        )\n",
    "    else:\n",
    "        d_fit_median = None\n",
    "        error_d_median = None\n",
    "        frac_uncert_median = None\n",
    "\n",
    "    return pd.Series({\n",
    "        \"nH\": nH_val,\n",
    "        \"d\": d_val,\n",
    "\n",
    "        \"red_chi_squared\": df['red_chi_squared'].median() if 'red_chi_squared' in df.columns else None,\n",
    "        \"gamma\": df['gamma'].median() if 'gamma' in df.columns else None,\n",
    "        \"temp\": df['temp'].median() if 'temp' in df.columns else None,\n",
    "\n",
    "        \"power_norm_fit\": df['power_norm_fit'].median() if 'power_norm_fit' in df.columns else None,\n",
    "        \"disk_norm_fit\": df['disk_norm_fit'].median() if 'disk_norm_fit' in df.columns else None,\n",
    "        \"total_flux_median\": df['total_flux'].median() if 'total_flux' in df.columns else None,\n",
    "\n",
    "        \"d_fit_median\": d_fit_median,\n",
    "        \"d_fit_peak\": d_fit_peak,\n",
    "        \"peak_flux\": total_flux_peak,\n",
    "        \"d_fit_weighted_median\": d_fit_weighted_med,\n",
    "\n",
    "        \"error_d_median\": error_d_median,\n",
    "        \"frac_uncert_median\": frac_uncert_median,\n",
    "        \"error_d_peak\": peak_dist_err,\n",
    "        \"frac_uncert_peak\": frac_peak_err\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Function to compute ONLY a subset of columns\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_subset_stats(df, needed_cols):\n",
    "    \"\"\"\n",
    "    Compute only the columns in `needed_cols` for each group (nH, d).\n",
    "    Return them plus (nH, d).\n",
    "    \"\"\"\n",
    "    nH_val = df['nH'].iloc[0] if 'nH' in df.columns else None\n",
    "    d_val  = df['d'].iloc[0]  if 'd'  in df.columns else None\n",
    "\n",
    "    result = {\n",
    "        \"nH\": nH_val,\n",
    "        \"d\": d_val,\n",
    "    }\n",
    "\n",
    "    # Example logic for 'd_fit_peak', 'error_d_peak', 'frac_uncert_peak'\n",
    "    if \"d_fit_peak\" in needed_cols or \"error_d_peak\" in needed_cols or \"frac_uncert_peak\" in needed_cols:\n",
    "        d_fit_peak = find_peak(df['d_fit'].dropna()) if 'd_fit' in df.columns else None\n",
    "        if \"d_fit_peak\" in needed_cols:\n",
    "            result[\"d_fit_peak\"] = d_fit_peak\n",
    "        if d_fit_peak is not None and d_val is not None:\n",
    "            peak_dist_err = d_fit_peak - d_val\n",
    "            frac_peak_err = peak_dist_err / d_val if d_val != 0 else None\n",
    "        else:\n",
    "            peak_dist_err = None\n",
    "            frac_peak_err = None\n",
    "        if \"error_d_peak\" in needed_cols:\n",
    "            result[\"error_d_peak\"] = peak_dist_err\n",
    "        if \"frac_uncert_peak\" in needed_cols:\n",
    "            result[\"frac_uncert_peak\"] = frac_peak_err\n",
    "\n",
    "    if \"peak_flux\" in needed_cols:\n",
    "        result[\"peak_flux\"] = find_peak(df['total_flux'].dropna()) if 'total_flux' in df.columns else None\n",
    "\n",
    "    if \"d_fit_weighted_median\" in needed_cols:\n",
    "        if {'d_fit', 'error_d_low', 'error_d_up'}.issubset(df.columns):\n",
    "            valid_mask = (\n",
    "                df['d_fit'].notna() &\n",
    "                df['error_d_low'].notna() &\n",
    "                df['error_d_up'].notna()\n",
    "            )\n",
    "            if valid_mask.any():\n",
    "                w = 1 / calc_MAE(\n",
    "                    df.loc[valid_mask, 'd_fit'],\n",
    "                    df.loc[valid_mask, 'error_d_low'],\n",
    "                    df.loc[valid_mask, 'error_d_up']\n",
    "                )\n",
    "                result[\"d_fit_weighted_median\"] = weighted_median(\n",
    "                    df.loc[valid_mask, 'd_fit'].values,\n",
    "                    w.values\n",
    "                )\n",
    "            else:\n",
    "                result[\"d_fit_weighted_median\"] = None\n",
    "        else:\n",
    "            result[\"d_fit_weighted_median\"] = None\n",
    "\n",
    "    if (\"d_fit_median\" in needed_cols or \n",
    "        \"error_d_median\" in needed_cols or \n",
    "        \"frac_uncert_median\" in needed_cols):\n",
    "        \n",
    "        if 'd_fit' in df.columns and df['d_fit'].notna().any() and d_val is not None:\n",
    "            d_fit_median = df['d_fit'].median()\n",
    "            error_d_median = d_fit_median - d_val\n",
    "            frac_uncert_median = (error_d_median / d_val) if d_val != 0 else None\n",
    "        else:\n",
    "            d_fit_median = None\n",
    "            error_d_median = None\n",
    "            frac_uncert_median = None\n",
    "\n",
    "        if \"d_fit_median\" in needed_cols:\n",
    "            result[\"d_fit_median\"] = d_fit_median\n",
    "        if \"error_d_median\" in needed_cols:\n",
    "            result[\"error_d_median\"] = error_d_median\n",
    "        if \"frac_uncert_median\" in needed_cols:\n",
    "            result[\"frac_uncert_median\"] = frac_uncert_median\n",
    "\n",
    "    if \"red_chi_squared\" in needed_cols:\n",
    "        result[\"red_chi_squared\"] = df['red_chi_squared'].median() if 'red_chi_squared' in df.columns else None\n",
    "\n",
    "    if \"gamma\" in needed_cols:\n",
    "        result[\"gamma\"] = df['gamma'].median() if 'gamma' in df.columns else None\n",
    "\n",
    "    if \"temp\" in needed_cols:\n",
    "        result[\"temp\"] = df['temp'].median() if 'temp' in df.columns else None\n",
    "\n",
    "    if \"power_norm_fit\" in needed_cols:\n",
    "        result[\"power_norm_fit\"] = df['power_norm_fit'].median() if 'power_norm_fit' in df.columns else None\n",
    "\n",
    "    if \"disk_norm_fit\" in needed_cols:\n",
    "        result[\"disk_norm_fit\"] = df['disk_norm_fit'].median() if 'disk_norm_fit' in df.columns else None\n",
    "\n",
    "    if \"total_flux_median\" in needed_cols:\n",
    "        result[\"total_flux_median\"] = df['total_flux'].median() if 'total_flux' in df.columns else None\n",
    "\n",
    "    return pd.Series(result)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Main Loop: Only Compute Missing Columns, Collect Data Structures\n",
    "# ---------------------------------------------------------------------\n",
    "merged_results = []\n",
    "\n",
    "for file in tqdm(csv_files_full, desc=\"Processing FULL CSV Files\"):\n",
    "    # 6a) Extract parameters\n",
    "    params = extract_parameters(file)\n",
    "    if params is None:\n",
    "        continue\n",
    "\n",
    "    base_name_full = os.path.basename(file)\n",
    "    base_name_no_ext = os.path.splitext(base_name_full)[0].replace(\"_full\", \"\")\n",
    "\n",
    "    # 6b) Attempt to find existing aggregated file\n",
    "    existing_agg = None\n",
    "    for f_agg in csv_files_agg:\n",
    "        if base_name_no_ext in f_agg:\n",
    "            try:\n",
    "                existing_agg = pd.read_csv(f_agg)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {f_agg}: {e}\")\n",
    "                existing_agg = None\n",
    "\n",
    "    # 6c) Load the FULL CSV data\n",
    "    try:\n",
    "        full_data = pd.read_csv(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Must have 'nH' and 'd' for grouping\n",
    "    if 'nH' not in full_data.columns or 'd' not in full_data.columns:\n",
    "        continue\n",
    "\n",
    "    # 6d) If no existing aggregated file, compute ALL columns\n",
    "    if existing_agg is None:\n",
    "        aggregated_df = (\n",
    "            full_data\n",
    "            .groupby(['nH', 'd'], as_index=False)\n",
    "            .apply(compute_all_stats)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        # Identify which columns are missing\n",
    "        missing_cols = [c for c in all_needed_cols if c not in existing_agg.columns]\n",
    "        if not missing_cols:\n",
    "            # Everything is already present\n",
    "            aggregated_df = existing_agg.copy()\n",
    "        else:\n",
    "            # Compute only the missing columns\n",
    "            partial_agg = (\n",
    "                full_data\n",
    "                .groupby(['nH', 'd'], as_index=False)\n",
    "                .apply(lambda grp: compute_subset_stats(grp, missing_cols))\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            # Merge partial_agg with existing_agg on (nH, d)\n",
    "            aggregated_df = pd.merge(\n",
    "                existing_agg,\n",
    "                partial_agg,\n",
    "                on=[\"nH\", \"d\"],\n",
    "                how=\"outer\",\n",
    "                suffixes=(\"_old\", \"\")\n",
    "            )\n",
    "            # Fill missing columns from partial_agg\n",
    "            for col in missing_cols:\n",
    "                old_col_name = col + \"_old\"\n",
    "                new_col_name = col\n",
    "                if old_col_name in aggregated_df.columns and new_col_name in aggregated_df.columns:\n",
    "                    # Fill the old column's NaNs with new column's values\n",
    "                    mask_na = aggregated_df[old_col_name].isna()\n",
    "                    aggregated_df.loc[mask_na, old_col_name] = aggregated_df.loc[mask_na, new_col_name]\n",
    "                    # Drop new col, rename old -> col\n",
    "                    aggregated_df.drop(columns=[new_col_name], inplace=True)\n",
    "                    aggregated_df.rename(columns={old_col_name: col}, inplace=True)\n",
    "                elif old_col_name in aggregated_df.columns and new_col_name not in aggregated_df.columns:\n",
    "                    # The new aggregator didn't produce that col, rename old -> new\n",
    "                    aggregated_df.rename(columns={old_col_name: col}, inplace=True)\n",
    "\n",
    "            # Remove leftover columns with _old suffix\n",
    "            leftover_old = [c for c in aggregated_df.columns if c.endswith(\"_old\")]\n",
    "            aggregated_df.drop(columns=leftover_old, inplace=True)\n",
    "\n",
    "    # 6e) Collect data for distances, na_distances, etc.\n",
    "    # Filter rows where `central_value` is present\n",
    "    valid_data = aggregated_df[aggregated_df[central_value].notna()]\n",
    "    distances.extend(valid_data['d'].values)\n",
    "\n",
    "    # Rows where `central_value` is NaN\n",
    "    na_data = aggregated_df[aggregated_df[central_value].isna()]\n",
    "    na_distances.extend(na_data['d'].values)\n",
    "    na_nH.extend(na_data['nH'].values)\n",
    "\n",
    "    # Keep the entire aggregated data in `all_data`\n",
    "    all_data.append(aggregated_df)\n",
    "    all_data_params.append(params)\n",
    "\n",
    "    # Check if 'd' == 1 for missing `central_value`\n",
    "    selected_data = aggregated_df[aggregated_df['d'] == 1]\n",
    "    if selected_data[central_value].isna().any():\n",
    "        na_d_fit_files.append(os.path.basename(file))\n",
    "        na_d_fit_params.append(params)\n",
    "\n",
    "    merged_results.append(aggregated_df)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) Combine everything (optional) and finalize\n",
    "# ---------------------------------------------------------------------\n",
    "final_merged_df = pd.concat(merged_results, ignore_index=True)\n",
    "\n",
    "# If you want to save the final result:\n",
    "# final_merged_df.to_csv(\"final_merged_aggregated_stats.csv\", index=False)\n",
    "\n",
    "print(\"Done. Only missing columns were computed for existing aggregated files.\")\n",
    "print(f\"Collected {len(distances)} valid distances and {len(na_distances)} missing distances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exp_decay(x,a,b):\n",
    "    return b*np.exp(-a*x)\n",
    "\n",
    "def volume_density_exp_decay(x,L):\n",
    "    return (1/(2*(L**3)))*(x**2)*np.exp(-(x/L))\n",
    "\n",
    "n, bins,_ = plt.hist(distances, bins=50, edgecolor='black',density = True)\n",
    "\n",
    "print(n.sum()*(bins[1]-bins[0]))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "xdata = bins[:-1] + ((bins[1:]-bins[:-1])/2) \n",
    "\n",
    "xdata = xdata[n>0]\n",
    "\n",
    "ydata = n[n>0]\n",
    "\n",
    "popt, pcov = curve_fit(volume_density_exp_decay, xdata, ydata)\n",
    "\n",
    "# distances_numpy = np.array(distances)\n",
    "# kde = KernelDensity(kernel='gaussian', bandwidth=5).fit(distances_numpy.reshape(-1, 1))\n",
    "distances_plot = np.linspace(0.215, 40, 3979)\n",
    "# density = np.exp(kde.score_samples(distances_plot.reshape(-1, 1)))\n",
    "\n",
    "density = volume_density_exp_decay(distances_plot,*popt)\n",
    "\n",
    "# density = density/(density.sum()*(distances_plot[1]-distances_plot[0]))\n",
    "\n",
    "print(density.sum()*(distances_plot[1]-distances_plot[0]))\n",
    "\n",
    "print((distances_plot[1]-distances_plot[0]))\n",
    "\n",
    "plt.plot(xdata,ydata)\n",
    "plt.plot(distances_plot,density)\n",
    "plt.xlabel('Distance (d)')\n",
    "plt.ylabel('Density')\n",
    "# plt.yscale('log')\n",
    "# plt.title('Histogram of Distances with Absolute Fractional Uncertainty < 0.5 (Aggregated)')\n",
    "np.save(f'{instr}_density.npy',density)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine distances and na_distances into a list of datasets\n",
    "data = [distances, na_distances]\n",
    "\n",
    "# Plot the histogram of the aggregated distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, bins=50, edgecolor='black', stacked=True, label=['Distances', 'Non-Available Distances'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Distance (d)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Stacked Histogram of Distances')\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the aggregated distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(distances, bins=50, edgecolor='black')\n",
    "plt.hist(na_distances, bins=50, edgecolor='red', alpha=0.5, label='Non-Available Distances')\n",
    "plt.xlabel('Distance (d)')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.yscale('log')\n",
    "plt.title('Histogram of Distances with Absolute Fractional Uncertainty < 0.5 (Aggregated)')\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the aggregated distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(na_distances, bins=50, edgecolor='black')\n",
    "plt.xlabel('Distance (d)')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Distances with Absolute Fractional Uncertainty < 0.5 (Aggregated)')\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the data into a single DataFrame\n",
    "aggregated_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "heatmap_data = aggregated_data.pivot_table(index='nH', columns='d', values=central_value, aggfunc=np.median)\n",
    "\n",
    "# Sort the index (nH) in ascending order\n",
    "heatmap_data = heatmap_data.sort_index(ascending=False)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, cmap='viridis', annot=True, cbar_kws={'label': central_value})\n",
    "plt.xlabel('Distance (d)') \n",
    "plt.ylabel('nH')\n",
    "plt.title('Heatmap of nH vs Distance with Fractional Uncertainty as Height')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group the data by 'nH' and 'd'\n",
    "grouped_data = aggregated_data.groupby(['nH', 'd'])\n",
    "\n",
    "# Define unique values of nH and d for the grid layout\n",
    "nH_values = sorted(aggregated_data['nH'].unique(), reverse=True)\n",
    "d_values = sorted(aggregated_data['d'].unique())\n",
    "\n",
    "# Create a color palette for nH values\n",
    "palette = sns.color_palette(\"tab10\", len(nH_values))  # Adjust palette as needed\n",
    "nH_colors = {nH: color for nH, color in zip(nH_values, palette)}\n",
    "\n",
    "# Adjust figure size to fit MNRAS page dimensions\n",
    "fig_width = 6.974  # MNRAS page width in inches\n",
    "fig_height_per_row = 1.8  # Adjusted height per row\n",
    "fig_height = fig_height_per_row * len(nH_values)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(nH_values), len(d_values),\n",
    "    figsize=(fig_width, fig_height),\n",
    "    sharex=True, sharey=True,  # Share x and y axes\n",
    "    gridspec_kw={'wspace': 0, 'hspace': 0}  # Overlap axes\n",
    ")\n",
    "\n",
    "# Get global y-axis limits\n",
    "all_d_fit_values = aggregated_data[central_value]\n",
    "\n",
    "# Iterate through each nH and d to plot the distribution at each grid cell\n",
    "for i, nH in enumerate(nH_values):\n",
    "    for j, d in enumerate(d_values):\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        # Get the distribution of d_fit for each combination of nH and d\n",
    "        subset = grouped_data.get_group((nH, d))[central_value] if (nH, d) in grouped_data.groups else []\n",
    "        \n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            sns.violinplot(\n",
    "                y=subset, ax=ax, inner=\"point\",\n",
    "                color=nH_colors[nH], linewidth=0.5  # Use the color for the current nH\n",
    "            )\n",
    "        \n",
    "        # Reduce grid lines to only major ticks on the y-axis\n",
    "        ax.grid(True, axis='y', which='major', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        # Horizontal line for reference\n",
    "        ax.axhline(y=d_values[j], color=\"black\", linestyle='-', linewidth=1.2)\n",
    "\n",
    "        # Set titles and labels\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"$D = {d}$ kpc\", fontsize=7, pad=5)  # Font size adjusted\n",
    "\n",
    "        if j == 0:  # Only add label once per row\n",
    "            ax.text(0.05, 0.95, f\"Input $N_H$ = {nH}\", transform=ax.transAxes,\n",
    "                    fontsize=5, verticalalignment='top', horizontalalignment='left',\n",
    "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.2',linewidth=0.3))\n",
    "\n",
    "        # Add x-axis labels for the bottom row\n",
    "        if i == len(nH_values) - 1:\n",
    "            ax.set_xlabel(f\"$D = {d}$ kpc\", fontsize=7)\n",
    "            ax.tick_params(axis='x', rotation=45)  # Rotate x-labels\n",
    "\n",
    "        # Customize ticks\n",
    "        ax.tick_params(axis='both', which='major', labelsize=6, length=5, width=0.8)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=6, length=3, width=0.5)\n",
    "\n",
    "# Shared ylabel positioned closer to the subplots\n",
    "fig.text(0.015, 0.5, 'Estimated $D$ (kpc)', va='center', rotation='vertical', fontsize=10)\n",
    "\n",
    "# Adjust layout to ensure no text overlap\n",
    "plt.tight_layout(pad=0.8, rect=[0.03, 0.05, 1, 0.95])\n",
    "\n",
    "# Save the figure as a high-resolution PDF\n",
    "plt.savefig(f\"plots/{instr}/2d_violin_plot_{instr}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Define the parameter labels with proper LaTeX formatting and units\n",
    "parameter_labels = {\n",
    "    'g': r\"$\\Gamma$\",                        # Gamma (no unit label)\n",
    "    'T': r\"$T$ (keV)\",                       # Temperature in keV\n",
    "    'a': r\"$a$\",                              # Spin parameter (no unit label)\n",
    "    'm': r\"$m$ ($M_{\\odot}$)\",                     # Mass in solar masses\n",
    "    'i': r\"$i$ (degrees)\",                   # Inclination in degrees\n",
    "    'r': r\"Disk-to-total ratio\",             # Disk to total ratio (no unit label)\n",
    "    'e': r\"Exposure (sec)\"                   # Exposure in seconds\n",
    "}\n",
    "\n",
    "# Step 1: Attach parameters to each entry in all_data using all_data_params\n",
    "all_data_with_params = []\n",
    "for data_df, params in zip(all_data, all_data_params):  # Match each dataset with its parameters\n",
    "    # Repeat the parameter dictionary for each row in the corresponding data_df\n",
    "    params_df = pd.DataFrame([params] * len(data_df))\n",
    "    # Concatenate data and parameters\n",
    "    data_with_params = pd.concat([data_df.reset_index(drop=True), params_df.reset_index(drop=True)], axis=1)\n",
    "    all_data_with_params.append(data_with_params)\n",
    "\n",
    "# Step 2: Concatenate all entries into a single DataFrame\n",
    "all_data_flat = pd.concat(all_data_with_params, ignore_index=True)\n",
    "\n",
    "# Define the parameters to aggregate by\n",
    "parameters = ['g', 'T', 'a', 'm', 'i', 'r', 'e']\n",
    "\n",
    "# Step 3: Loop through each parameter and create violin plots\n",
    "for param in parameters:\n",
    "    # Get unique values for the current parameter and distance\n",
    "    param_values = sorted(all_data_flat[param].unique())\n",
    "    d_values = sorted(all_data_flat['d'].unique())\n",
    "\n",
    "    # Create subplots for the current parameter\n",
    "    fig, axes = plt.subplots(\n",
    "        len(param_values), 1,\n",
    "        figsize=(3.3, len(param_values) * 1.5),  # Single-column layout\n",
    "        sharex=True\n",
    "    )\n",
    "\n",
    "    # Iterate through each parameter value and plot distributions\n",
    "    for i, (ax, val) in enumerate(zip(axes, param_values)):\n",
    "        # Filter the data for the current parameter value\n",
    "        subset = all_data_flat.loc[all_data_flat[param] == val].copy()\n",
    "\n",
    "        if not subset.empty:\n",
    "            subset['d'] = subset['d'].astype(str)  # Convert 'd' to string for categorical x-axis\n",
    "            ax.set_yscale('log')  # Use log scale for y-axis\n",
    "            palette = sns.color_palette('colorblind', len(d_values))  # Colorblind-friendly palette\n",
    "            sns.violinplot(\n",
    "                data=subset,\n",
    "                x='d',\n",
    "                y=central_value,\n",
    "                ax=ax,\n",
    "                hue='d',  # Assign hue to the same as x-axis\n",
    "                inner=None,  # No internal marks\n",
    "                palette=palette,\n",
    "                linewidth=0.3,  # Reduced line width for clarity\n",
    "                legend=False  # Suppress legend\n",
    "            )\n",
    "\n",
    "            # Dynamically calculate the true x-axis limits after padding\n",
    "            x_limits = ax.get_xlim()\n",
    "            x_left = x_limits[0]  # True left edge of the axis\n",
    "            for d_idx, d_value in enumerate(d_values):\n",
    "                # Get subset of data for the current `d_value`\n",
    "                data_d_value = subset.loc[subset['d'] == str(d_value), central_value]\n",
    "                if not data_d_value.empty:\n",
    "                    x_max = d_idx + 0.3  # Slightly after the violin's center\n",
    "                    ax.plot([x_left, x_max], [d_value, d_value], color=palette[d_idx], linestyle='-', linewidth=0.5)\n",
    "\n",
    "        # Add titles using parameter_labels\n",
    "        ax.set_title(f\"{parameter_labels[param]} = {val}\", fontsize=7, pad=2)\n",
    "        ax.set_ylabel(\"Estimated $D$ (kpc)\", fontsize=7, labelpad=2)\n",
    "        ax.grid(True, which='major', linestyle=':', linewidth=0.3)\n",
    "\n",
    "        # Adjust tick size to match label font size\n",
    "        ax.tick_params(axis='both', which='major', labelsize=7, length=3, width=0.3)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=7, length=2, width=0.3)\n",
    "\n",
    "        # Adjust the y-axis ticks explicitly for log scaling\n",
    "        ax.yaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=5))  # Max 5 major ticks\n",
    "        ax.yaxis.set_minor_locator(ticker.LogLocator(base=10.0, subs=\"auto\", numticks=10))  # Minor ticks\n",
    "        ax.set_xlim(x_limits)\n",
    "\n",
    "    # Add x-axis label for the bottom plot\n",
    "    axes[-1].set_xlabel(\"$D$ (kpc)\", fontsize=7)\n",
    "\n",
    "    # Adjust layout to ensure no overlaps and save the figure\n",
    "    plt.tight_layout(pad=0.8)\n",
    "    plt.savefig(f\"plots/{instr}/{param}_violin_plot_{instr}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each parameter in files that have 'd_fit' = NaN\n",
    "plt.figure(figsize=(15, 10))\n",
    "parameter_names = ['g', 'T', 'a', 'm', 'i', 'r', 'e']\n",
    "\n",
    "# Iterate over parameters and plot histograms in a 2x3 layout\n",
    "for idx, param_name in enumerate(parameter_names[:7]):  # Only using the first 6 parameters for the 2x3 layout\n",
    "    values = [params[param_name] for params in na_d_fit_params if param_name in params]\n",
    "    plt.subplot(3, 3, idx + 1)\n",
    "    plt.hist(values, bins=15, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram for Parameter: {param_name} (Files with d_fit = NaN)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges\n",
    "x_bins = [1, 2, 3, 4, 5, 6, 8, 12, 18, 27]\n",
    "y_bins = [0, 0.5,1, 6.7, 10]\n",
    "\n",
    "# Plot a 2D histogram for 'nH' and 'd' of non-available distances\n",
    "plt.figure(figsize=(12, 6))\n",
    "hist, x_edges, y_edges, im = plt.hist2d(na_distances, na_nH, bins=(x_bins, y_bins), cmap='viridis')\n",
    "plt.ylabel('nH')\n",
    "plt.xlabel('Distance (d)')\n",
    "plt.colorbar(label='Count')\n",
    "plt.title('2D Histogram of nH and Distance for Non-Available Distances')\n",
    "\n",
    "# Annotate counts on the cells\n",
    "for i in range(len(x_edges) - 1):\n",
    "    for j in range(len(y_edges) - 1):\n",
    "        count = hist[i, j]\n",
    "        # if count > 0:  # Only annotate non-empty bins\n",
    "        plt.text((x_edges[i] + x_edges[i + 1]) / 2,\n",
    "                    (y_edges[j] + y_edges[j + 1]) / 2,\n",
    "                    int(count),\n",
    "                    ha='center', va='center', color='white')\n",
    "                    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
